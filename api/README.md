# üöÄ PDF RAG API - Documenta√ß√£o

API REST para consulta de documentos PDF usando RAG (Retrieval-Augmented Generation). Ideal para integra√ß√£o com agentes de IA e sistemas externos.

## üìã √çndice

- [Instala√ß√£o](#instala√ß√£o)
- [Iniciando a API](#iniciando-a-api)
- [Endpoints](#endpoints)
- [Exemplos de Uso](#exemplos-de-uso)
- [Cliente Python](#cliente-python)
- [Integra√ß√£o com Agentes de IA](#integra√ß√£o-com-agentes-de-ia)
- [Autentica√ß√£o](#autentica√ß√£o)
- [Rate Limiting](#rate-limiting)

## üîß Instala√ß√£o

### 1. Instalar depend√™ncias adicionais

```bash
# Com uv (recomendado)
uv pip install fastapi uvicorn pydantic requests

# Ou com pip
pip install fastapi uvicorn[standard] pydantic requests
```

### 2. Estrutura de diret√≥rios

```bash
# Crie o diret√≥rio da API
mkdir api examples

# Crie __init__.py
touch api/__init__.py examples/__init__.py
```

### 3. Copiar arquivos

Copie os seguintes arquivos para a estrutura:

- `api/main.py` - Aplica√ß√£o FastAPI principal
- `api/models.py` - Modelos Pydantic
- `api/dependencies.py` - Inje√ß√£o de depend√™ncias
- `api/chat_manager.py` - Gerenciador de sess√µes
- `examples/api_client.py` - Cliente exemplo

## üöÄ Iniciando a API

### Op√ß√£o 1: Comando direto

```bash
python api/main.py
```

### Op√ß√£o 2: Uvicorn (recomendado para produ√ß√£o)

```bash
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

### Op√ß√£o 3: Com configura√ß√µes customizadas

```bash
uvicorn api.main:app \
  --host 0.0.0.0 \
  --port 8000 \
  --workers 4 \
  --log-level info
```

**Sa√≠da esperada:**
```
üöÄ Iniciando PDF RAG API...
‚úì Configura√ß√£o carregada
‚úì VectorStore inicializado
‚úì RAG Engine inicializado
‚úì Chat Manager inicializado
‚úì 1250 chunks indexados
‚úÖ API pronta para receber requisi√ß√µes

INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

### Acessar documenta√ß√£o interativa

- **Swagger UI:** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc

## üì° Endpoints

### 1. Health Check

**GET** `/health`

Verifica status da API e componentes.

```bash
curl http://localhost:8000/health
```

**Response:**
```json
{
  "status": "healthy",
  "components": {
    "vectorstore": "ok",
    "rag_engine": "ok",
    "chat_manager": "ok"
  },
  "total_documents": 1250
}
```

### 2. Estat√≠sticas

**GET** `/stats`

Retorna estat√≠sticas do banco vetorial.

```bash
curl http://localhost:8000/stats
```

**Response:**
```json
{
  "total_chunks": 1250,
  "unique_sources": 5,
  "sources": [
    "manual.pdf",
    "guide.pdf",
    "handbook.pdf"
  ],
  "collection_name": "pdf_documents"
}
```

### 3. Query (Principal)

**POST** `/query`

Faz uma pergunta e retorna resposta baseada nos documentos.

**Request:**
```json
{
  "question": "O que √© machine learning?",
  "k": 6,
  "include_sources": true
}
```

**Response:**
```json
{
  "question": "O que √© machine learning?",
  "answer": "Machine learning √© um subcampo da intelig√™ncia artificial que permite que sistemas aprendam e melhorem a partir da experi√™ncia sem serem explicitamente programados...",
  "sources": [
    {
      "source": "ai_handbook.pdf",
      "page": 12,
      "title": "AI Handbook",
      "excerpt": "Machine learning permite que sistemas..."
    }
  ],
  "num_sources": 4
}
```

**Exemplo cURL:**
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "O que √© machine learning?",
    "k": 6,
    "include_sources": true
  }'
```

### 4. Search

**POST** `/search`

Busca chunks similares sem gerar resposta.

**Request:**
```json
{
  "query": "neural networks",
  "k": 5,
  "filter": {
    "source": "ml_guide.pdf"
  }
}
```

**Response:**
```json
{
  "query": "neural networks",
  "chunks": [
    {
      "content": "Redes neurais s√£o modelos computacionais inspirados no c√©rebro humano...",
      "metadata": {
        "source": "ml_guide.pdf",
        "page": 8,
        "title": "ML Guide",
        "chunk_id": 5
      }
    }
  ],
  "total_results": 5
}
```

### 5. Chat

**POST** `/chat`

Chat conversacional com hist√≥rico de sess√£o.

**Request:**
```json
{
  "session_id": "user-123",
  "message": "O que s√£o transformers?",
  "k": 6
}
```

**Response:**
```json
{
  "session_id": "user-123",
  "message": "O que s√£o transformers?",
  "response": "Transformers s√£o uma arquitetura de rede neural introduzida em 2017...",
  "sources": [...],
  "num_sources": 4
}
```

### 6. Chat History

**GET** `/chat/{session_id}/history`

Retorna hist√≥rico de uma sess√£o.

```bash
curl http://localhost:8000/chat/user-123/history
```

### 7. Clear Chat Session

**DELETE** `/chat/{session_id}`

Limpa o hist√≥rico de uma sess√£o.

```bash
curl -X DELETE http://localhost:8000/chat/user-123
```

## üí° Exemplos de Uso

### Python com requests

```python
import requests

# Query simples
response = requests.post(
    "http://localhost:8000/query",
    json={
        "question": "Qual √© o tema principal?",
        "k": 6,
        "include_sources": True
    }
)

result = response.json()
print(result['answer'])
```

### JavaScript/TypeScript

```javascript
// Query com fetch
const response = await fetch('http://localhost:8000/query', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    question: 'O que √© deep learning?',
    k: 6,
    include_sources: true
  })
});

const data = await response.json();
console.log(data.answer);
```

### cURL

```bash
# Query b√°sica
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"question": "Explique redes neurais", "k": 6}'

# Busca com filtro
curl -X POST http://localhost:8000/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "deep learning",
    "k": 5,
    "filter": {"source": "manual.pdf"}
  }'

# Chat
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "user-456",
    "message": "Ol√°, pode me ajudar?",
    "k": 6
  }'
```

## ü§ñ Cliente Python

Usando o cliente fornecido:

```python
from examples.api_client import PDFRAGClient

# Inicializa cliente
client = PDFRAGClient(base_url="http://localhost:8000")

# Verifica sa√∫de
health = client.health_check()
print(health)

# Faz pergunta
result = client.query(
    question="O que √© intelig√™ncia artificial?",
    k=6,
    include_sources=True
)
print(result['answer'])

# Busca chunks
chunks = client.search(
    query="machine learning",
    k=5
)

# Chat conversacional
response = client.chat(
    session_id="meu-usuario",
    message="Explique redes neurais"
)
print(response['response'])
```

### Executar exemplos

```bash
# Certifique-se que a API est√° rodando
python api/main.py

# Em outro terminal
python examples/api_client.py
```

## ü§ñ Integra√ß√£o com Agentes de IA

### Exemplo: Agente LangChain

```python
from langchain.tools import Tool
from examples.api_client import PDFRAGClient

# Cria cliente
client = PDFRAGClient()

# Define ferramenta para o agente
pdf_search_tool = Tool(
    name="PDF Knowledge Base",
    func=lambda q: client.query(q, k=6)['answer'],
    description="Busca informa√ß√µes na base de conhecimento de documentos PDF. Use esta ferramenta quando precisar de informa√ß√µes espec√≠ficas dos documentos."
)

# Usar no agente
from langchain.agents import initialize_agent, AgentType
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(temperature=0)
agent = initialize_agent(
    tools=[pdf_search_tool],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# Executar
response = agent.run("Pesquise sobre machine learning nos documentos")
```

### Exemplo: Workflow de Atendimento ao Cliente

```python
class CustomerServiceAgent:
    def __init__(self):
        self.pdf_client = PDFRAGClient()
    
    def handle_customer_query(self, customer_id: str, question: str):
        """Responde pergunta do cliente usando documentos"""
        
        # 1. Buscar contexto relevante
        search_result = self.pdf_client.search(
            query=question,
            k=5
        )
        
        # 2. Gerar resposta contextualizada
        response = self.pdf_client.chat(
            session_id=customer_id,
            message=question,
            k=6
        )
        
        # 3. Formatar resposta para o cliente
        return {
            "answer": response['response'],
            "confidence": "high" if response['num_sources'] > 3 else "medium",
            "sources": [s['source'] for s in response['sources'][:2]]
        }

# Uso
agent = CustomerServiceAgent()
result = agent.handle_customer_query(
    customer_id="customer-789",
    question="Como fa√ßo para configurar o produto?"
)
```

### Exemplo: RAG com OpenAI Function Calling

```python
import openai
from examples.api_client import PDFRAGClient

client = PDFRAGClient()

# Define fun√ß√£o para OpenAI
functions = [
    {
        "name": "search_documents",
        "description": "Busca informa√ß√µes em documentos PDF da base de conhecimento",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Pergunta ou t√≥pico a buscar"
                }
            },
            "required": ["query"]
        }
    }
]

# Chamada com function calling
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "Me explique sobre deep learning"}
    ],
    functions=functions,
    function_call="auto"
)

# Se chamou a fun√ß√£o, executa
if response.choices[0].message.get("function_call"):
    function_args = json.loads(
        response.choices[0].message.function_call.arguments
    )
    
    # Busca nos documentos
    docs_result = client.query(
        question=function_args["query"],
        k=6
    )
    
    # Envia resultado de volta
    final_response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "user", "content": "Me explique sobre deep learning"},
            response.choices[0].message,
            {
                "role": "function",
                "name": "search_documents",
                "content": json.dumps(docs_result)
            }
        ]
    )
    
    print(final_response.choices[0].message.content)
```

## üîê Autentica√ß√£o (Recomendado para Produ√ß√£o)

Para adicionar autentica√ß√£o b√°sica:

```python
# Em api/main.py, adicione:

from fastapi import Security, HTTPException
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):
    """Verifica token de autentica√ß√£o"""
    token = credentials.credentials
    
    # Valide seu token aqui
    if token != "seu-token-secreto":
        raise HTTPException(
            status_code=401,
            detail="Token inv√°lido"
        )
    return token

# Adicione como depend√™ncia nos endpoints
@app.post("/query")
async def query_documents(
    request: QueryRequest,
    rag: RAGEngine = Depends(get_rag_engine),
    token: str = Depends(verify_token)  # ‚Üê Adiciona autentica√ß√£o
):
    ...
```

## ‚ö° Rate Limiting (Recomendado para Produ√ß√£o)

```bash
# Instalar slowapi
uv pip install slowapi

# Adicionar em api/main.py
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/query")
@limiter.limit("10/minute")  # 10 requests por minuto
async def query_documents(request: Request, ...):
    ...
```

## üê≥ Deploy com Docker

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instala depend√™ncias
COPY requirements_api.txt .
RUN pip install -r requirements_api.txt

# Copia c√≥digo
COPY . .

# Exp√µe porta
EXPOSE 8000

# Comando de inicializa√ß√£o
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./data:/app/data
    restart: unless-stopped
```

```bash
# Build e run
docker-compose up -d

# Logs
docker-compose logs -f api
```

## üìä Monitoramento

### Prometheus + Grafana

```python
# Instalar
uv pip install prometheus-fastapi-instrumentator

# Em api/main.py
from prometheus_fastapi_instrumentator import Instrumentator

Instrumentator().instrument(app).expose(app)
```

## üß™ Testes da API

```python
# tests/test_api.py
import pytest
from fastapi.testclient import TestClient
from api.main import app

client = TestClient(app)

def test_health_check():
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"

def test_query():
    response = client.post(
        "/query",
        json={
            "question": "Teste",
            "k": 3,
            "include_sources": True
        }
    )
    assert response.status_code == 200
    assert "answer" in response.json()

# Executar
pytest tests/test_api.py -v
```

## üìû Suporte

Para problemas ou d√∫vidas:

- **Issues:** [GitHub Issues](https://github.com/seu-repo/issues)
- **Documenta√ß√£o:** http://localhost:8000/docs
- **Email:** suporte@seu-dominio.com

---

**Vers√£o:** 1.0.0  
**√öltima Atualiza√ß√£o:** Dezembro 2025